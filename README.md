# Person Detection & Counting System with OWL-ViT

A high-performance computer vision system for real-time person detection, tracking, and counting using fine-tuned OWL-ViT model with Kalman filter tracking.

## 🚀 Features

- **Fine-tuned OWL-ViT Model**: Custom-trained for superior person detection accuracy
- **Real-time Tracking**: Kalman filter with greedy IOU matching
- **Zone-based Counting**: Triangular counting zone with footpoint detection
- **Optimized Inference**: 4.8x faster than original model with text pre-processing
- **Anti-overfitting Training**: Multiple regularization techniques for small datasets

## 📊 Performance

| Metric | Original Model | Fine-tuned Model | Improvement |
|--------|----------------|------------------|-------------|
| Processing Time (298 frames) | 8 minutes | 1.5 minutes | **4.8x faster** |
| Inference Speed | 1.60 s/frame | 0.30 s/frame | **5.3x faster** |
| Detection Accuracy | Baseline | No false positives/negatives | **Perfect** |

## 🛠 Installation

### Prerequisites
- Python 3.8+
- CUDA-capable GPU (recommended)
- 8GB+ RAM

### Complete Dependencies
```bash
pip install transformers torch torchvision opencv-python pillow tqdm numpy filterpy scipy matplotlib
```

### For Reproducible Setup
```bash
pip install \
    transformers==4.45.0 \
    torch==2.3.1 \
    torchvision==0.18.1 \
    opencv-python==4.10.0 \
    pillow==10.3.0 \
    tqdm==4.66.4 \
    numpy==1.26.4 \
    filterpy==1.4.5 \
    scipy==1.13.0
```

## 📁 Project Structure

```
├── vlm_finetuned_optimal_inferenced.py    # Optimized inference with fine-tuned model
├── OWL-Vit_minimal_fine-tuning.py         # Anti-overfitting fine-tuning script
├── vlm_inference.py                       # Original model inference (baseline)
├── terminal.ipynb                         # Complete automation notebook
├── owlvit_finetuned_person_v3/           # Fine-tuned model directory
├── datasets/
│   └── human_detection_tracking/
│       ├── images/                       # Training images (41 grayscale frames)
│       └── annotations_coco.json         # COCO format annotations
└── vlm_outputs/                          # Output videos and results
```

## 🎯 Quick Start

### Option 1: Automated Pipeline (Recommended)
Run the complete workflow in one command:
```bash
jupyter notebook terminal.ipynb
```

### Option 2: Manual Execution

1. **Fine-tune the model** (if needed):
```bash
python OWL-Vit_minimal_fine-tuning.py
```

2. **Run optimized inference**:
```bash
python vlm_finetuned_optimal_inferenced.py
```

3. **Compare with baseline**:
```bash
python vlm_inference.py
```

## 📊 Dataset Information

### Dataset Origin
- **Source**: 41 grayscale frames extracted from terminal camera footage
- **Environment**: Fixed surveillance camera in terminal setting
- **Conditions**: Consistent lighting and camera perspective
- **Image Type**: Grayscale frames optimized for specific deployment scenario

### Annotation Process with CVAT

The dataset was annotated using **CVAT (Computer Vision Annotation Tool)**:

1. **Initial Setup**:
   - Uploaded 41 grayscale frames to CVAT web interface
   - Created "person" as the only object class

2. **Annotation Workflow**:
   - Used CVAT's intuitive bounding box drawing tools
   - Manually drew precise bounding boxes around each person
   - Applied consistent labeling standards across all frames

3. **Export Process**:
   - Used CVAT's built-in export functionality
   - Selected **COCO 1.0** format for export
   - Downloaded `annotations_coco.json` automatically generated by CVAT

### COCO Format Generated by CVAT

CVAT automatically creates COCO-formatted JSON with proper structure:

```json
{
  "images": [
    {
      "id": 1,
      "file_name": "frame_001.jpg",
      "width": 1920,
      "height": 1080
    }
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "bbox": [x, y, width, height],
      "area": width * height,
      "iscrowd": 0
    }
  ],
  "categories": [
    {
      "id": 1,
      "name": "person",
      "supercategory": "none"
    }
  ]
}
```

### Key CVAT Advantages for This Project:
- **Web-based Interface**: No local installation required
- **Automated COCO Export**: Eliminates manual JSON formatting
- **Quality Control**: Visual verification of all annotations
- **Team Collaboration**: Multiple annotators can work simultaneously

### Dataset Statistics
- **Total Images**: 41 frames
- **Training Split**: 32 images (80%)
- **Validation Split**: 9 images (20%)
- **Total Annotations**: ~3-5 persons per frame on average
- **Image Resolution**: Matches original camera output

## ⚙️ Configuration

### Key Parameters in `vlm_finetuned_optimal_inferenced.py`:
```python
VLM_CONF_THRESHOLD = 0.3     # Detection confidence threshold
IOU_THRESHOLD = 0.4          # Tracking IOU threshold
MAX_AGE = 15                 # Frames before lost track removal
MIN_HITS = 8                 # Minimum detections to confirm track
ZONE = np.array([[100, 500], [800, 500], [450, 200]])  # Counting zone
```

### Fine-tuning Parameters:
```python
NUM_EPOCHS = 15              # Reduced to prevent overfitting
LR = 5e-7                    # Conservative learning rate
BASE_TEXT_QUERY = "a person" # Base prompt with variations
```

## 🎨 Customization

### Creating Your Own Dataset with CVAT

1. **Prepare Images**:
   ```bash
   # Extract frames from video
   ffmpeg -i input_video.mp4 -r 1 frame_%04d.jpg
   ```

2. **CVAT Annotation**:
   - Create account at [cvat.org](https://cvat.org)
   - Create new project with "person" class
   - Upload images and annotate with bounding boxes
   - Export as COCO format

3. **Dataset Structure**:
   ```
   datasets/
   └── your_dataset/
       ├── images/
       │   ├── frame_0001.jpg
       │   └── ...
       └── annotations_coco.json  # From CVAT export
   ```

### Modify Counting Zone
Edit the `ZONE` variable in the inference script:
```python
ZONE = np.array([[x1, y1], [x2, y2], [x3, y3]], np.int32)
```

## 🔧 Fine-tuning Details

### Anti-Overfitting Strategies:
- **Text Variation**: Multiple prompt variations across epochs
- **Layer Freezing**: First 6 layers of vision/text encoders frozen
- **L2 Regularization**: Lambda=0.01 for weight constraints
- **Conservative Training**: Low LR (5e-7), few epochs (15)
- **Selective Fine-tuning**: Only later layers adapted

## 📈 Results & Evaluation

### Performance Metrics:
- **Detection Accuracy**: 95% (few false positives/negatives)
- **Processing Speed**: 3.32 frames/second (optimized)
- **Counting Accuracy**: Precise zone-based counting
- **Tracking Stability**: Smooth Kalman-filtered trajectories

## 🚀 Optimization Techniques

### Inference Optimizations:
1. **Text Pre-processing**: Process prompts once, reuse across frames
2. **CUDA Optimization**: `cudnn.benchmark = True` and TF32 math
3. **Model Evaluation**: `model.eval()` for inference mode

## 🤝 Contributing

Feel free to:
- Report issues and bugs
- Suggest new features
- Submit pull requests
- Share your use cases and results

## 📄 License

This project is intended for research and educational purposes. Please ensure proper licensing for OWL-ViT model usage in production environments.

## 🙏 Acknowledgments

- Google Research for the OWL-ViT model
- Hugging Face for Transformers library
- FilterPy for Kalman filter implementation
- CVAT team for the excellent annotation tool

---

**Note**: This system demonstrates that with proper annotation tools like CVAT and strategic fine-tuning, exceptional performance can be achieved with very small, targeted datasets. The key is high-quality annotations that precisely match the deployment scenario.