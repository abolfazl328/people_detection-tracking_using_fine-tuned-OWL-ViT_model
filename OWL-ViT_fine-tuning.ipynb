{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import (\n",
    "    OwlViTProcessor,\n",
    "    OwlViTForObjectDetection,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_scheduler,\n",
    ")"
   ],
   "id": "cc84e4b5b7fae44f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DATASET_PATH = \"./datasets/human_detection_tracking\"\n",
    "ANNOTATION_FILE = \"annotations_coco.json\"\n",
    "MODEL_CHECKPOINT = \"google/owlvit-base-patch16\"\n",
    "OUTPUT_DIR = \"./owlvit_finetuned_person_v3\""
   ],
   "id": "c977009c11f8e785"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_EPOCHS = 15\n",
    "LR = 5e-7\n",
    "WARMUP_RATIO = 0.1"
   ],
   "id": "1e28738be65a89c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "BASE_TEXT_QUERY = \"a person\"",
   "id": "afaef2acc251d111"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(os.path.join(DATASET_PATH, ANNOTATION_FILE), \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "    \n",
    "images_info = coco[\"images\"]\n",
    "annotations = coco[\"annotations\"]\n",
    "\n",
    "ann_by_img_id = {}\n",
    "for ann in annotations:\n",
    "    ann_by_img_id.setdefault(ann[\"image_id\"], []).append(ann)"
   ],
   "id": "f99560ee003d3f68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_info, val_info = train_test_split(images_info, test_size=0.2, random_state=42)",
   "id": "41766270e3b46ead"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CocoPersonDataset(Dataset):\n",
    "    def __init__(self, image_info_list, annotations_dict, image_base_path, processor):\n",
    "        self.image_info_list = image_info_list\n",
    "        self.annotations_dict = annotations_dict\n",
    "        self.image_base_path = image_base_path\n",
    "        self.processor = processor\n",
    "        # Different text queries to prevent overfitting\n",
    "        self.text_variations = [\n",
    "            \"a person\", \n",
    "            \"human\", \n",
    "            \"person walking\", \n",
    "            \"a human\", \n",
    "            \"person\"\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_info_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_info = self.image_info_list[idx]\n",
    "        \n",
    "        # Fix the file path\n",
    "        file_name = image_info[\"file_name\"]\n",
    "        if file_name.startswith(\"images/\"):\n",
    "            file_name = file_name.replace(\"images/\", \"\", 1)\n",
    "        \n",
    "        image_path = os.path.join(self.image_base_path, file_name)\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            alt_path = os.path.join(self.image_base_path, \"images\", image_info[\"file_name\"])\n",
    "            if os.path.exists(alt_path):\n",
    "                image_path = alt_path\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        anns = copy.deepcopy(self.annotations_dict.get(image_info[\"id\"], []))\n",
    "\n",
    "        # Prepare bounding boxes and labels\n",
    "        boxes, labels = [], []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to normalized coordinates [0, 1]\n",
    "            image_width, image_height = image.size\n",
    "            boxes.append([x/image_width, y/image_height, (x + w)/image_width, (y + h)/image_height])\n",
    "            labels.append(0)  # person class\n",
    "\n",
    "        # Use different text queries to prevent overfitting\n",
    "        # Use modulo to cycle through variations based on index\n",
    "        text_query = [self.text_variations[idx % len(self.text_variations)]]\n",
    "\n",
    "        # Process image and text\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=text_query,  # Single text query per sample\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for k in encoding:\n",
    "            encoding[k] = encoding[k].squeeze(0)\n",
    "\n",
    "        # Add target boxes and labels (normalized coordinates)\n",
    "        encoding[\"target_boxes\"] = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        encoding[\"target_labels\"] = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "\n",
    "        return encoding"
   ],
   "id": "547775ff472a0661"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "processor = OwlViTProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "train_dataset = CocoPersonDataset(train_info, ann_by_img_id, os.path.join(DATASET_PATH, \"images\"), processor)\n",
    "val_dataset = CocoPersonDataset(val_info, ann_by_img_id, os.path.join(DATASET_PATH, \"images\"), processor)"
   ],
   "id": "dcfb7862233b4ce8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def data_collator(batch):\n",
    "    batch_out = {}\n",
    "    \n",
    "    # Stack image inputs\n",
    "    batch_out[\"pixel_values\"] = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    batch_out[\"input_ids\"] = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    batch_out[\"attention_mask\"] = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    \n",
    "    # Collect targets\n",
    "    batch_out[\"target_boxes\"] = [item[\"target_boxes\"] for item in batch]\n",
    "    batch_out[\"target_labels\"] = [item[\"target_labels\"] for item in batch]\n",
    "    \n",
    "    return batch_out"
   ],
   "id": "d8e9f7ae60a1059b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model = OwlViTForObjectDetection.from_pretrained(MODEL_CHECKPOINT)",
   "id": "af6ba99fe769ca97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"vision_model.encoder.layers\" in name:\n",
    "        layer_num = int(name.split(\".layers.\")[1].split(\".\")[0])\n",
    "        if layer_num < 6:  # Freeze first 6 layers of vision encoder\n",
    "            param.requires_grad = False\n",
    "    elif \"text_model.encoder.layers\" in name:\n",
    "        layer_num = int(name.split(\".layers.\")[1].split(\".\")[0])\n",
    "        if layer_num < 6:  # Freeze first 6 layers of text encoder\n",
    "            param.requires_grad = False"
   ],
   "id": "7c6d08f6fa07486d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=LR,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")"
   ],
   "id": "da7cb3365fde3905"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")"
   ],
   "id": "fac89e51ffa2933d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class OWLTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.box_loss_fn = nn.SmoothL1Loss(beta=0.1)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Extract targets\n",
    "        target_boxes = inputs.pop(\"target_boxes\")\n",
    "        target_labels = inputs.pop(\"target_labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "        )\n",
    "        \n",
    "        # Compute detection loss with L2 regularization\n",
    "        loss = self.compute_detection_loss(outputs, target_boxes, target_labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def compute_detection_loss(self, outputs, target_boxes, target_labels):\n",
    "        total_loss = 0.0\n",
    "        batch_size = len(target_boxes)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            pred_boxes = outputs.pred_boxes[i]  # (num_queries, 4)\n",
    "            pred_logits = outputs.logits[i]     # (num_queries, num_classes)\n",
    "            \n",
    "            tgt_boxes = target_boxes[i]\n",
    "            tgt_labels = target_labels[i]\n",
    "            \n",
    "            if len(tgt_boxes) == 0:\n",
    "                # If no targets, use background loss\n",
    "                bg_loss = self.compute_background_loss(pred_logits)\n",
    "                total_loss += bg_loss\n",
    "                continue\n",
    "            \n",
    "            # For each target, find best matching prediction\n",
    "            box_losses = []\n",
    "            for tgt_box in tgt_boxes:\n",
    "                # Compute MSE between target box and all predictions\n",
    "                box_diffs = torch.nn.functional.mse_loss(\n",
    "                    pred_boxes, \n",
    "                    tgt_box.unsqueeze(0).expand_as(pred_boxes), \n",
    "                    reduction='none'\n",
    "                ).mean(dim=1)\n",
    "                \n",
    "                # Use the minimum loss (best matching prediction)\n",
    "                min_box_loss = torch.min(box_diffs)\n",
    "                box_losses.append(min_box_loss)\n",
    "            \n",
    "            # Classification loss - encourage high scores for person class\n",
    "            person_scores = torch.sigmoid(pred_logits[:, 0])  # Person class scores\n",
    "            cls_loss = torch.nn.functional.binary_cross_entropy(\n",
    "                person_scores,\n",
    "                torch.ones_like(person_scores)  # Target: all should detect person\n",
    "            )\n",
    "            \n",
    "            # Combine losses\n",
    "            if box_losses:\n",
    "                avg_box_loss = sum(box_losses) / len(box_losses)\n",
    "                total_loss += avg_box_loss + cls_loss * 0.1  # Weight classification lower\n",
    "            else:\n",
    "                total_loss += cls_loss\n",
    "        \n",
    "        # Average over batch\n",
    "        if batch_size > 0:\n",
    "            total_loss = total_loss / batch_size\n",
    "        \n",
    "        # ADD L2 REGULARIZATION TO PREVENT OVERFITTING\n",
    "        l2_lambda = 0.01  # Regularization strength\n",
    "        l2_reg = torch.tensor(0.).to(total_loss.device)\n",
    "        for param in self.model.parameters():\n",
    "            if param.requires_grad:\n",
    "                l2_reg += torch.norm(param)  # L2 norm of weights\n",
    "        \n",
    "        total_loss = total_loss + l2_lambda * l2_reg\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_background_loss(self, pred_logits):\n",
    "        \"\"\"Loss for images with no objects - encourage low confidence\"\"\"\n",
    "        person_scores = torch.sigmoid(pred_logits[:, 0])\n",
    "        bg_loss = torch.nn.functional.binary_cross_entropy(\n",
    "            person_scores,\n",
    "            torch.zeros_like(person_scores)  # Target: no persons\n",
    "        )\n",
    "        return bg_loss * 0.1  # Lower weight for background\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        inputs = inputs.copy()\n",
    "        inputs.pop(\"target_boxes\", None)\n",
    "        inputs.pop(\"target_labels\", None)\n",
    "        return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)"
   ],
   "id": "4299dc8ee00e77f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_training_steps = len(train_dataset) // (2 * training_args.gradient_accumulation_steps) * NUM_EPOCHS\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ],
   "id": "e8ed78c17599961e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = OWLTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    ")\n"
   ],
   "id": "c1936c87d2a74b93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Starting ANTI-OVERFITTING fine-tuning...\")\n",
    "print(\"Applied modifications:\")\n",
    "print(\"1. Multiple text variations across samples: ['a person', 'human', 'person walking', 'a human', 'person']\")\n",
    "print(\"2. L2 regularization (lambda=0.01)\")\n",
    "print(\"3. Reduced epochs: 15 (from 20)\")\n",
    "print(\"4. Lower learning rate: 5e-7 (from 1e-6)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"✅ Anti-overfitting fine-tuning complete! Model saved to\", OUTPUT_DIR)"
   ],
   "id": "791469d647ea7648"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
