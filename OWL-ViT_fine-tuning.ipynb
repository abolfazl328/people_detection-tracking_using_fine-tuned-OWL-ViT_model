{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# OWL-ViT Fine-tuning for Person Detection\n",
    "\n",
    "This notebook demonstrates how to fine-tune the OWL-ViT model for person detection with anti-overfitting techniques. The model is trained on a small dataset of 41 grayscale frames from a terminal camera.\n",
    "\n",
    "## Key Features:\n",
    "- Anti-overfitting strategies for small datasets\n",
    "- Multiple text prompt variations\n",
    "- Layer freezing and L2 regularization\n",
    "- Conservative training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformers-imports",
   "metadata": {},
   "source": [
    "## 2. Import Transformers Components\n",
    "\n",
    "Import the OWL-ViT model, processor, and training utilities from Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transformers-imports-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    OwlViTProcessor,\n",
    "    OwlViTForObjectDetection,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configuration",
   "metadata": {},
   "source": [
    "## 3. Configuration Settings\n",
    "\n",
    "Set up paths and model configuration. The dataset should be in COCO format with person annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-settings",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./datasets/human_detection_tracking\"\n",
    "ANNOTATION_FILE = \"annotations_coco.json\"\n",
    "MODEL_CHECKPOINT = \"google/owlvit-base-patch16\"\n",
    "OUTPUT_DIR = \"./owlvit_finetuned_person_v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-params",
   "metadata": {},
   "source": [
    "## 4. Anti-Overfitting Training Parameters\n",
    "\n",
    "Conservative settings to prevent overfitting on small datasets:\n",
    "- **Low learning rate**: 5e-7\n",
    "- **Few epochs**: 15\n",
    "- **Warmup ratio**: 10% of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-parameters",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "LR = 5e-7\n",
    "WARMUP_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-query",
   "metadata": {},
   "source": [
    "## 5. Base Text Query\n",
    "\n",
    "The base text prompt for person detection. Multiple variations will be used during training to prevent overfitting to a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base-text-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TEXT_QUERY = \"a person\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-coco",
   "metadata": {},
   "source": [
    "## 6. Load COCO Annotations\n",
    "\n",
    "Load and parse the COCO format annotations created using CVAT annotation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-coco-annotations",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATASET_PATH, ANNOTATION_FILE), \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "    \n",
    "images_info = coco[\"images\"]\n",
    "annotations = coco[\"annotations\"]\n",
    "\n",
    "# Organize annotations by image ID for easy access\n",
    "ann_by_img_id = {}\n",
    "for ann in annotations:\n",
    "    ann_by_img_id.setdefault(ann[\"image_id\"], []).append(ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-test-split",
   "metadata": {},
   "source": [
    "## 7. Train-Validation Split\n",
    "\n",
    "Split the dataset into training (80%) and validation (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-val-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info, val_info = train_test_split(images_info, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-class",
   "metadata": {},
   "source": [
    "## 8. Custom Dataset Class\n",
    "\n",
    "Creates a PyTorch Dataset that:\n",
    "- Loads images and annotations\n",
    "- Converts bounding boxes to normalized coordinates\n",
    "- Uses multiple text variations to prevent overfitting\n",
    "- Processes images and text for OWL-ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoPersonDataset(Dataset):\n",
    "    def __init__(self, image_info_list, annotations_dict, image_base_path, processor):\n",
    "        self.image_info_list = image_info_list\n",
    "        self.annotations_dict = annotations_dict\n",
    "        self.image_base_path = image_base_path\n",
    "        self.processor = processor\n",
    "        # Different text queries to prevent overfitting\n",
    "        self.text_variations = [\n",
    "            \"a person\", \n",
    "            \"human\", \n",
    "            \"person walking\", \n",
    "            \"a human\", \n",
    "            \"person\"\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_info_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_info = self.image_info_list[idx]\n",
    "        \n",
    "        # Fix the file path\n",
    "        file_name = image_info[\"file_name\"]\n",
    "        if file_name.startswith(\"images/\"):\n",
    "            file_name = file_name.replace(\"images/\", \"\", 1)\n",
    "        \n",
    "        image_path = os.path.join(self.image_base_path, file_name)\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            alt_path = os.path.join(self.image_base_path, \"images\", image_info[\"file_name\"])\n",
    "            if os.path.exists(alt_path):\n",
    "                image_path = alt_path\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        anns = copy.deepcopy(self.annotations_dict.get(image_info[\"id\"], []))\n",
    "\n",
    "        # Prepare bounding boxes and labels\n",
    "        boxes, labels = [], []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            # Convert to normalized coordinates [0, 1]\n",
    "            image_width, image_height = image.size\n",
    "            boxes.append([x/image_width, y/image_height, (x + w)/image_width, (y + h)/image_height])\n",
    "            labels.append(0)  # person class\n",
    "\n",
    "        # Use different text queries to prevent overfitting\n",
    "        # Use modulo to cycle through variations based on index\n",
    "        text_query = [self.text_variations[idx % len(self.text_variations)]]\n",
    "\n",
    "        # Process image and text\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=text_query,  # Single text query per sample\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for k in encoding:\n",
    "            encoding[k] = encoding[k].squeeze(0)\n",
    "\n",
    "        # Add target boxes and labels (normalized coordinates)\n",
    "        encoding[\"target_boxes\"] = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        encoding[\"target_labels\"] = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros(0, dtype=torch.int64)\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processor-datasets",
   "metadata": {},
   "source": [
    "## 9. Initialize Processor and Datasets\n",
    "\n",
    "Load the OWL-ViT processor and create training/validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-processor-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = OwlViTProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "train_dataset = CocoPersonDataset(train_info, ann_by_img_id, os.path.join(DATASET_PATH, \"images\"), processor)\n",
    "val_dataset = CocoPersonDataset(val_info, ann_by_img_id, os.path.join(DATASET_PATH, \"images\"), processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-collator",
   "metadata": {},
   "source": [
    "## 10. Data Collator\n",
    "\n",
    "Custom data collator to properly batch the data for training. Handles variable numbers of bounding boxes per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-collator-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    batch_out = {}\n",
    "    \n",
    "    # Stack image inputs\n",
    "    batch_out[\"pixel_values\"] = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    batch_out[\"input_ids\"] = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    batch_out[\"attention_mask\"] = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    \n",
    "    # Collect targets (can have variable lengths)\n",
    "    batch_out[\"target_boxes\"] = [item[\"target_boxes\"] for item in batch]\n",
    "    batch_out[\"target_labels\"] = [item[\"target_labels\"] for item in batch]\n",
    "    \n",
    "    return batch_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-model",
   "metadata": {},
   "source": [
    "## 11. Load Pre-trained Model\n",
    "\n",
    "Load the base OWL-ViT model for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OwlViTForObjectDetection.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layer-freezing",
   "metadata": {},
   "source": [
    "## 12. Selective Layer Freezing\n",
    "\n",
    "Freeze early layers of both vision and text encoders to prevent overfitting and reduce trainable parameters:\n",
    "- Freeze first 6 layers of vision encoder\n",
    "- Freeze first 6 layers of text encoder\n",
    "- Only fine-tune later layers for adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "layer-freezing-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"vision_model.encoder.layers\" in name:\n",
    "        layer_num = int(name.split(\".layers.\")[1].split(\".\")[0])\n",
    "        if layer_num < 6:  # Freeze first 6 layers of vision encoder\n",
    "            param.requires_grad = False\n",
    "    elif \"text_model.encoder.layers\" in name:\n",
    "        layer_num = int(name.split(\".layers.\")[1].split(\".\")[0])\n",
    "        if layer_num < 6:  # Freeze first 6 layers of text encoder\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimizer",
   "metadata": {},
   "source": [
    "## 13. Optimizer Setup\n",
    "\n",
    "Configure AdamW optimizer with weight decay for regularization. Only optimizes unfrozen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimizer-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=LR,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-args",
   "metadata": {},
   "source": [
    "## 14. Training Arguments\n",
    "\n",
    "Configure training parameters with anti-overfitting settings:\n",
    "- Small batch sizes with gradient accumulation\n",
    "- Mixed precision (FP16) for faster training\n",
    "- Gradient clipping and warmup\n",
    "- Conservative learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-args-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-trainer",
   "metadata": {},
   "source": [
    "## 15. Custom Trainer Class\n",
    "\n",
    "Implements a custom training loop with:\n",
    "- Custom loss function for object detection\n",
    "- L2 regularization for additional overfitting prevention\n",
    "- Background loss for images with no persons\n",
    "- Box regression and classification losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-trainer-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OWLTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.box_loss_fn = nn.SmoothL1Loss(beta=0.1)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Extract targets\n",
    "        target_boxes = inputs.pop(\"target_boxes\")\n",
    "        target_labels = inputs.pop(\"target_labels\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "        )\n",
    "        \n",
    "        # Compute detection loss with L2 regularization\n",
    "        loss = self.compute_detection_loss(outputs, target_boxes, target_labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def compute_detection_loss(self, outputs, target_boxes, target_labels):\n",
    "        total_loss = 0.0\n",
    "        batch_size = len(target_boxes)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            pred_boxes = outputs.pred_boxes[i]  # (num_queries, 4)\n",
    "            pred_logits = outputs.logits[i]     # (num_queries, num_classes)\n",
    "            \n",
    "            tgt_boxes = target_boxes[i]\n",
    "            tgt_labels = target_labels[i]\n",
    "            \n",
    "            if len(tgt_boxes) == 0:\n",
    "                # If no targets, use background loss\n",
    "                bg_loss = self.compute_background_loss(pred_logits)\n",
    "                total_loss += bg_loss\n",
    "                continue\n",
    "            \n",
    "            # For each target, find best matching prediction\n",
    "            box_losses = []\n",
    "            for tgt_box in tgt_boxes:\n",
    "                # Compute MSE between target box and all predictions\n",
    "                box_diffs = torch.nn.functional.mse_loss(\n",
    "                    pred_boxes, \n",
    "                    tgt_box.unsqueeze(0).expand_as(pred_boxes), \n",
    "                    reduction='none'\n",
    "                ).mean(dim=1)\n",
    "                \n",
    "                # Use the minimum loss (best matching prediction)\n",
    "                min_box_loss = torch.min(box_diffs)\n",
    "                box_losses.append(min_box_loss)\n",
    "            \n",
    "            # Classification loss - encourage high scores for person class\n",
    "            person_scores = torch.sigmoid(pred_logits[:, 0])  # Person class scores\n",
    "            cls_loss = torch.nn.functional.binary_cross_entropy(\n",
    "                person_scores,\n",
    "                torch.ones_like(person_scores)  # Target: all should detect person\n",
    "            )\n",
    "            \n",
    "            # Combine losses\n",
    "            if box_losses:\n",
    "                avg_box_loss = sum(box_losses) / len(box_losses)\n",
    "                total_loss += avg_box_loss + cls_loss * 0.1  # Weight classification lower\n",
    "            else:\n",
    "                total_loss += cls_loss\n",
    "        \n",
    "        # Average over batch\n",
    "        if batch_size > 0:\n",
    "            total_loss = total_loss / batch_size\n",
    "        \n",
    "        # ADD L2 REGULARIZATION TO PREVENT OVERFITTING\n",
    "        l2_lambda = 0.01  # Regularization strength\n",
    "        l2_reg = torch.tensor(0.).to(total_loss.device)\n",
    "        for param in self.model.parameters():\n",
    "            if param.requires_grad:\n",
    "                l2_reg += torch.norm(param)  # L2 norm of weights\n",
    "        \n",
    "        total_loss = total_loss + l2_lambda * l2_reg\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def compute_background_loss(self, pred_logits):\n",
    "        \"\"\"Loss for images with no objects - encourage low confidence\"\"\"\n",
    "        person_scores = torch.sigmoid(pred_logits[:, 0])\n",
    "        bg_loss = torch.nn.functional.binary_cross_entropy(\n",
    "            person_scores,\n",
    "            torch.zeros_like(person_scores)  # Target: no persons\n",
    "        )\n",
    "        return bg_loss * 0.1  # Lower weight for background\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        inputs = inputs.copy()\n",
    "        inputs.pop(\"target_boxes\", None)\n",
    "        inputs.pop(\"target_labels\", None)\n",
    "        return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduler",
   "metadata": {},
   "source": [
    "## 16. Learning Rate Scheduler\n",
    "\n",
    "Configure cosine learning rate scheduler with warmup for smooth training convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduler-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = len(train_dataset) // (2 * training_args.gradient_accumulation_steps) * NUM_EPOCHS\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer-init",
   "metadata": {},
   "source": [
    "## 17. Initialize Trainer\n",
    "\n",
    "Create the trainer instance with all components: model, datasets, data collator, optimizer, and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer-init-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = OWLTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution",
   "metadata": {},
   "source": [
    "## 18. Start Fine-tuning\n",
    "\n",
    "Begin the anti-overfitting fine-tuning process and save the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-execution-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting ANTI-OVERFITTING fine-tuning...\")\n",
    "print(\"Applied modifications:\")\n",
    "print(\"1. Multiple text variations across samples: ['a person', 'human', 'person walking', 'a human', 'person']\")\n",
    "print(\"2. L2 regularization (lambda=0.01)\")\n",
    "print(\"3. Reduced epochs: 15 (from 20)\")\n",
    "print(\"4. Lower learning rate: 5e-7 (from 1e-6)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"âœ… Anti-overfitting fine-tuning complete! Model saved to\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}