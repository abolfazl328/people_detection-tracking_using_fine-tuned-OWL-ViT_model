{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from filterpy.kalman import KalmanFilter\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from PIL import Image"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "FINE_TUNED_MODEL_PATH = \"./owlvit_finetuned_person_v3\"\n",
    "TEXT_PROMPTS = [\"a person\"]\n",
    "VLM_CONF_THRESHOLD = 0.3\n",
    "\n",
    "IOU_THRESHOLD = 0.4\n",
    "MAX_AGE = 15\n",
    "MIN_HITS = 8\n",
    "\n",
    "INPUT_VIDEO_PATH = './input.mp4'\n",
    "OUTPUT_VIDEO_PATH = f'./vlm_outputs/vlm_finetuned_v3_fast_output_{VLM_CONF_THRESHOLD}_{IOU_THRESHOLD}_{MAX_AGE}_{MIN_HITS}.mp4'\n",
    "\n",
    "ZONE = np.array([[100, 500], [800, 500], [450, 200]], np.int32)"
   ],
   "id": "1b6c60eacfe48369"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class KalmanTracker:\n",
    "    def __init__(self):\n",
    "        self.next_track_id = 0\n",
    "        self.tracks = {}\n",
    "\n",
    "    def update(self, boxes):\n",
    "        # Predict next state for all existing tracks\n",
    "        for track_id, track in self.tracks.items():\n",
    "            track['kf'].predict()\n",
    "            track['age'] += 1\n",
    "\n",
    "        # Match detections to existing tracks using IOU\n",
    "        matched_indices = []\n",
    "        if len(boxes) > 0 and len(self.tracks) > 0:\n",
    "            track_ids = list(self.tracks.keys())\n",
    "            track_boxes = np.array([t['kf'].x[:4, 0] for t in self.tracks.values()])\n",
    "            \n",
    "            for i, box in enumerate(boxes):\n",
    "                max_iou = 0\n",
    "                best_match = -1\n",
    "                for j, track_box in enumerate(track_boxes):\n",
    "                    iou = self.iou(box, [track_box[0], track_box[1], track_box[0]+track_box[2], track_box[1]+track_box[3]])\n",
    "                    if iou > max_iou:\n",
    "                        max_iou = iou\n",
    "                        best_match = j\n",
    "                \n",
    "                if max_iou > IOU_THRESHOLD:\n",
    "                    track_id = track_ids[best_match]\n",
    "                    self.tracks[track_id]['kf'].update(np.array([box[0], box[1], box[2]-box[0], box[3]-box[1]]).reshape(4,1))\n",
    "                    self.tracks[track_id]['age'] = 0\n",
    "                    self.tracks[track_id]['hits'] += 1\n",
    "                    matched_indices.append(i)\n",
    "\n",
    "        # Create new tracks for unmatched detections\n",
    "        for i, box in enumerate(boxes):\n",
    "            if i not in matched_indices:\n",
    "                kf = self.create_kalman_filter()\n",
    "                kf.x[:4] = np.array([box[0], box[1], box[2]-box[0], box[3]-box[1]]).reshape(4,1)\n",
    "                self.tracks[self.next_track_id] = {'kf': kf, 'age': 0, 'hits': 1}\n",
    "                self.next_track_id += 1\n",
    "\n",
    "        # Remove old tracks\n",
    "        dead_tracks = [track_id for track_id, track in self.tracks.items() if track['age'] > MAX_AGE]\n",
    "        for track_id in dead_tracks:\n",
    "            del self.tracks[track_id]\n",
    "\n",
    "        # Return active tracks\n",
    "        active_tracks = {}\n",
    "        for track_id, track in self.tracks.items():\n",
    "            if track['hits'] >= MIN_HITS:\n",
    "                pos = track['kf'].x\n",
    "                active_tracks[track_id] = [pos[0,0], pos[1,0], pos[0,0]+pos[2,0], pos[1,0]+pos[3,0]]\n",
    "        \n",
    "        return active_tracks\n",
    "\n",
    "    def create_kalman_filter(self):\n",
    "        kf = KalmanFilter(dim_x=8, dim_z=4)\n",
    "        kf.F = np.array([[1,0,0,0,1,0,0,0], [0,1,0,0,0,1,0,0], [0,0,1,0,0,0,1,0], [0,0,0,1,0,0,0,1],\n",
    "                         [0,0,0,0,1,0,0,0], [0,0,0,0,0,1,0,0], [0,0,0,0,0,0,1,0], [0,0,0,0,0,0,0,1]])\n",
    "        kf.H = np.array([[1,0,0,0,0,0,0,0], [0,1,0,0,0,0,0,0], [0,0,1,0,0,0,0,0], [0,0,0,1,0,0,0,0]])\n",
    "        kf.R *= 10.\n",
    "        kf.P *= 1000.\n",
    "        kf.Q *= 0.01\n",
    "        return kf\n",
    "\n",
    "    def iou(self, bbox1, bbox2):\n",
    "        x1_1, y1_1, x2_1, y2_1 = bbox1\n",
    "        x1_2, y1_2, x2_2, y2_2 = bbox2\n",
    "        \n",
    "        xi1 = max(x1_1, x1_2)\n",
    "        yi1 = max(y1_1, y1_2)\n",
    "        xi2 = min(x2_1, x2_2)\n",
    "        yi2 = min(y2_1, y2_2)\n",
    "        inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "        box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "        box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        \n",
    "        return inter_area / union_area if union_area != 0 else 0"
   ],
   "id": "8d995a624296c857"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading FINE-TUNED OWL-ViT model...\")\n",
    "processor = OwlViTProcessor.from_pretrained(FINE_TUNED_MODEL_PATH)\n",
    "model = OwlViTForObjectDetection.from_pretrained(FINE_TUNED_MODEL_PATH)"
   ],
   "id": "a587fe965bd8427f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.backends.cudnn.benchmark = True  # Optimizes CUDA operations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Faster matrix math\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ],
   "id": "1e3fe34bc8972b61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    text_inputs = processor(text=TEXT_PROMPTS, return_tensors=\"pt\").to(device)\n",
    "    # Keep text inputs on GPU\n",
    "    text_inputs = {k: v for k, v in text_inputs.items()}"
   ],
   "id": "3c6ef4ed2d733b07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tracker = SmartTracker()\n",
    "cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n",
    "\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
    "person_counter = 0\n",
    "frames_inside_zone = defaultdict(int)\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
   ],
   "id": "f0421c9b7e9cb669"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with tqdm(total=total_frames, desc=\"Processing video (OPTIMIZED)\") as pbar:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success: \n",
    "            break\n",
    "            \n",
    "        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # OPTIMIZED INFERENCE: Process image only, reuse text inputs\n",
    "        with torch.no_grad():\n",
    "            # Process image (this is fast)\n",
    "            image_inputs = processor(images=pil_image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Combine with pre-processed text inputs\n",
    "            inputs = {**text_inputs, **image_inputs}\n",
    "            \n",
    "            # Run model - this is where most time is spent\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Post-processing\n",
    "        target_sizes = torch.Tensor([pil_image.size[::-1]]).to(device)\n",
    "        vlm_results = processor.post_process_object_detection(\n",
    "            outputs=outputs, \n",
    "            target_sizes=target_sizes, \n",
    "            threshold=VLM_CONF_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        boxes = vlm_results[0][\"boxes\"].cpu().numpy()\n",
    "        scores = vlm_results[0][\"scores\"].cpu().numpy()\n",
    "        labels = vlm_results[0][\"labels\"].cpu().numpy()\n",
    "        \n",
    "        # Filter for person detections only\n",
    "        person_boxes = []\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            if label == 0 and score >= VLM_CONF_THRESHOLD:\n",
    "                person_boxes.append(box)\n",
    "        \n",
    "        person_boxes = np.array(person_boxes) if person_boxes else np.array([])\n",
    "        \n",
    "        # Update tracker\n",
    "        tracked_objects = tracker.update(person_boxes)\n",
    "        \n",
    "        # Draw results (same as before)\n",
    "        for object_id, box in tracked_objects.items():\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'ID: {object_id}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            footpoint = (int((x1 + x2) / 2), y2)\n",
    "            is_inside = cv2.pointPolygonTest(ZONE, footpoint, False) >= 0\n",
    "            \n",
    "            if is_inside:\n",
    "                frames_inside_zone[object_id] += 1\n",
    "                if frames_inside_zone[object_id] == 10: \n",
    "                    person_counter += 1\n",
    "                    frames_inside_zone[object_id] = -100\n",
    "            else:\n",
    "                frames_inside_zone[object_id] = max(0, frames_inside_zone[object_id])\n",
    "        \n",
    "        cv2.polylines(frame, [ZONE], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        cv2.putText(frame, f'Count: {person_counter}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)\n",
    "        cv2.putText(frame, 'Fine-tuned (Full Quality)', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(frame)\n",
    "        pbar.update(1)"
   ],
   "id": "de02c21d12798546"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Processing complete. Final video saved to: {OUTPUT_VIDEO_PATH}\")\n",
    "print(f\"Total persons counted: {person_counter}\")"
   ],
   "id": "d06b8e6fb775d058"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
